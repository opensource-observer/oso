apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: production-trino
spec:
  values:
    service:
      annotations:
        tailscale.com/expose: "true"
    env:
      - name: TRINO_GCS_KEY_ID
        value: gcp:secretmanager:production-mcs-gcs-key-id/versions/latest
      - name: TRINO_GCS_SECRET
        value: gcp:secretmanager:production-mcs-gcs-secret/versions/latest
      - name: CLICKHOUSE_URL
        value: gcp:secretmanager:production-trino-clickhouse-url/versions/latest
      - name: CLICKHOUSE_USER
        value: gcp:secretmanager:production-trino-clickhouse-user/versions/latest
      - name: TRINO_GCP_CREDENTIALS_JSON
        value: gcp:secretmanager:production-trino-gcp-credentials-json/versions/latest
      - name: CLICKHOUSE_PASSWORD
        value: gcp:secretmanager:production-trino-clickhouse-password/versions/latest
    serviceAccount:
      name: production-trino
    coordinator:
      resources:
        requests:
          cpu: 7300m
          memory: 46000Mi
      tolerations:
        - key: pool_type
          operator: Equal
          value: trino-coordinator
          effect: NoSchedule
      nodeSelector:
        pool_type: trino-coordinator
      jvm:
        maxHeapSize: "17G"
      additionalJVMConfig:
        - "--add-opens=java.base/java.nio=ALL-UNNAMED"

    worker:
      resources:
        requests:
          cpu: 78000m
          memory: 1700Gi
      tolerations:
        - key: pool_type
          operator: Equal
          value: trino-worker
          effect: NoSchedule
      nodeSelector:
        pool_type: trino-worker
      config:
        query:
          maxMemoryPerNode: 700GB
      jvm:
        maxHeapSize: "1500G"
      additionalJVMConfig:
        - "--add-opens=java.base/java.nio=ALL-UNNAMED"

    additionalConfigProperties:
      - retry-policy=TASK
      # Set a very large max query length ONLY for internal writer production
      # trino
      - "query.max-length=1000000000"
      - "query.max-stage-count=200"
      - "exchange.http-client.max-content-length=64MB"
    additionalExchangeManagerProperties:
      - "exchange.sink-buffers-per-partition=2"
      - "exchange.sink-buffer-pool-min-size=10"
      - "exchange.source-concurrent-readers=2"
      - "exchange.max-page-storage-size=64MB"
      - "exchange.s3.region=us-central1"
      - "exchange.s3.aws-access-key=${ENV:TRINO_GCS_KEY_ID}"
      - "exchange.s3.aws-secret-key=${ENV:TRINO_GCS_SECRET}"
      - "exchange.s3.endpoint=https://storage.googleapis.com"
      
    server:
      exchangeManager:
        name: filesystem
        baseDir: gs://oso-iceberg-exchange-usc1/trino-exchange
      config:
        query:
          maxMemory: "7000GB"
      workers: 1
      autoscaling:
        enabled: true
        maxReplicas: 10
        targetCPUUtilizationPercentage: 30
        behavior:
          scaleDown:
            stabilizationWindowSeconds: 600
            policies:
            - type: Pods
              value: 1
              periodSeconds: 120
          scaleUp:
            stabilizationWindowSeconds: 60
            policies:
            - type: Percent
              value: 100
              periodSeconds: 60
            - type: Pods
              value: 2 
              periodSeconds: 60
            selectPolicy: Max
    catalogs:
      iceberg: |
        connector.name=iceberg
        iceberg.max-commit-retry=10
        iceberg.catalog.type=rest
        iceberg.rest-catalog.uri=http://production-nessie.production-nessie.svc.cluster.local:19120/iceberg
        iceberg.rest-catalog.prefix=restate
        iceberg.rest-catalog.warehouse=gs://oso-iceberg-usc1/warehouse/
        iceberg.use-file-size-from-metadata=false
        iceberg.max-partitions-per-writer=1000
        fs.native-gcs.enabled=true
        gcs.project-id=opensource-observer
        parquet.max-buffer-size=32768MB
      source: |
        connector.name=hive
        hive.metastore.uri=thrift://10.145.192.30:9083
        fs.native-gcs.enabled=true
        gcs.project-id=opensource-observer
        hive.non-managed-table-writes-enabled=true
      bigquery: |
        connector.name=bigquery
        bigquery.project-id=opensource-observer
        bigquery.arrow-serialization.max-allocation=65536MB
      bigquery_public_data: |
        connector.name=bigquery
        bigquery.parent-project-id=opensource-observer
        bigquery.project-id=bigquery-public-data
        bigquery.credentials-key=${ENV:TRINO_GCP_CREDENTIALS_JSON}
        bigquery.case-insensitive-name-matching=true
        bigquery.arrow-serialization.max-allocation=65536MB
      github_archive: |
        connector.name=bigquery
        bigquery.parent-project-id=opensource-observer
        bigquery.project-id=githubarchive
        bigquery.credentials-key=${ENV:TRINO_GCP_CREDENTIALS_JSON}
        bigquery.arrow-serialization.max-allocation=65536MB
      clickhouse: |
        connector.name=clickhouse
        connection-url=${ENV:CLICKHOUSE_URL}
        connection-user=${ENV:CLICKHOUSE_USER}
        connection-password=${ENV:CLICKHOUSE_PASSWORD}
        clickhouse.map-string-as-varchar=true