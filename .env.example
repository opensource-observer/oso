# .env
## This .env file is mostly used for Python data ops

## Dagster Setup
# You may want to change the location of dagster home if you want it to survive resets 
DAGSTER_HOME=/tmp/dagster-home 

## sqlmesh
SQLMESH_DUCKDB_LOCAL_PATH=/tmp/oso.duckdb
#SQLMESH_DUCKDB_LOCAL_TRINO_PATH=/tmp/oso-trino.duckdb

# Uncomment the next two vars to use gcp secrets (you'll need to have gcp
# secrets configured). Unfortunately at this time, if you don't have access to
# the official oso gcp account uncommenting these will likely not work. The GCP
# secrets prefix should likely match the dagster deployment's search prefix in flux
DAGSTER_USE_LOCAL_SECRETS=True
#DAGSTER_GCP_SECRETS_PREFIX=dagster 

# OSO's python libraries are configured to use json logging by default but this
# can be annoying when viewing things locally. This will configure logs to be
# output in a more human-readable format.
OSO_ENABLE_JSON_LOGS=0

## Google Cloud setup
# You will need to generate Google application credentials.
# You can log in via `gcloud auth application-default login`
# Then you can enter the path to your credentials
# e.g. /home/user/.config/gcloud/application_default_credentials.json
GOOGLE_APPLICATION_CREDENTIALS=
# GCP project ID
GOOGLE_PROJECT_ID=
# Used for storing all BigQuery data in the dbt pipeline
BIGQUERY_DATASET_ID=
# Used when loading dlt assets into a staging area. It should be set to a GCS
# bucket that will be used to write to for dlt data transfers into bigquery.
DAGSTER_STAGING_BUCKET_URL=gs://some-bucket

## Clickhouse setup
DAGSTER__CLICKHOUSE__HOST=
DAGSTER__CLICKHOUSE__USER=
DAGSTER__CLICKHOUSE__PASSWORD=

# MCP + Text2SQL Agent
AGENT_VECTOR_STORE__TYPE=local
AGENT_VECTOR_STORE__DIR=/path/to/your/vector/storage/directory

AGENT_LLM__TYPE=google_genai
AGENT_LLM__GOOGLE_API_KEY=your_google_genai_api_key_here

AGENT_OSO_API_KEY=your_oso_api_key_here
AGENT_ARIZE_PHOENIX_USE_CLOUD=0

###################
# DEPRECATED
###################

# This is used to put generated dbt profiles for dagster in a specific place
DAGSTER_DBT_TARGET_BASE_DIR=/tmp/dagster-home/generated-dbt
DAGSTER_DBT_PARSE_PROJECT_ON_LOAD=0

# Used for data transfer between databases
CLOUDSTORAGE_BUCKET_NAME=

# Used for Frontend/API-facing services
CLOUDSQL_REGION=
CLOUDSQL_INSTANCE_ID=
CLOUDSQL_DB_NAME=
CLOUDSQL_DB_PASSWORD=
CLOUDSQL_DB_USER=

# Setup agent
AGENT_OSO_API_KEY=
# If using a local llm, this will attempt to use ollama with llama3.2:3b
# See warehouse/oso_agent/util/config.py for more options
AGENT_LLM__TYPE=local

# To use google's gen ai llm uncomment and add your google api key from google ai studio
#AGENT_LLM__TYPE=google_genai
#AGENT_LLM__GOOGLE_API_KEY=

AGENT_VECTOR_STORE__TYPE=local
# If using the local vector store, set this to a local directory for storing the
# vector store on disk
AGENT_VECTOR_STORE__DIR=

# For google vertex ai vector search, set the following options
#AGENT_VECTOR_STORE__TYPE=google_genai
#AGENT_VECTOR_STORE__GCS_BUCKET=
#AGENT_VECTOR_STORE__PROJECT_ID=
#AGENT_VECTOR_STORE__INDEX_ID=
#AGENT_VECTOR_STORE__ENDPOINT_ID=